package Basic

import org.apache.spark.{SPARK_BRANCH, SparkConf}
import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.{Seconds, StreamingContext}

import scala.util.matching.Regex

case class Log(timestamp:String, job_id:String, event_type:String, message:String)

object LogEvent {
  /**
   * Scala Main Function
   * @param args Array[String]
   */
  def main(args: Array[String]): Unit = {

    val sparkConf = new SparkConf().setAppName("Read Log Files generated by Flume").setMaster("local[2]")
    val ssc = new StreamingContext(sparkConf, Seconds(10))

    // Define regular expression : <date_time> <log_type> <message>
    val regex:Regex = raw"^([\\d-]+ [\\d:]+) ([a-zA-Z]+) (.*)".r

    // Read only today's data
    val lines = ssc.textFileStream("use_cases/streaming/events/" + java.time.LocalDate.now)

    // Apply regex and create a case class from filter matches
    val df = lines.map(regex.findAllIn(_)).filter(_.isEmpty == false).map(matches => Log(matches.group(1), matches.group(2), matches.group(3), matches.group(4)))

    // Actions
    df.print()
    df.saveAsObjectFiles("FlumeStreaming/flumeoutput")


    /**
     * Start Streaming and Await for Termination
     */
    ssc.start()
    ssc.awaitTermination()
  }
}